# EssayFlow - Documentation Index

Complete guide to the EssayFlow handwritten essay processing system.

## ğŸ“š Documentation Files

### Getting Started
1. **[QUICKSTART.md](QUICKSTART.md)** â­ START HERE
   - 3-minute setup guide
   - First submission walkthrough
   - Basic troubleshooting

2. **[README.md](README.md)**
   - Project overview
   - Features and capabilities
   - Setup instructions
   - Usage guide

### Technical Documentation
3. **[ARCHITECTURE.md](ARCHITECTURE.md)**
   - Pipeline architecture
   - Data flow diagrams
   - Module responsibilities
   - Design principles
   - Extension points

4. **[PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)**
   - What was built
   - File structure
   - Key features
   - Data models
   - Next steps

5. **[TESTING_GUIDE.md](TESTING_GUIDE.md)**
   - Manual testing procedures
   - Automated test structure
   - Test cases for real OCR
   - Performance testing
   - Edge cases

## ğŸ—‚ï¸ Source Code Files

### Main Application
- **`app.py`** - Streamlit web interface
  - File upload handling
  - Processing workflow
  - Results display
  - CSV export UI

### Pipeline Modules (`pipeline/`)

#### Core Data Models
- **`schema.py`** - Pydantic models
  - `SubmissionRecord` - Complete submission data
  - `OcrResult` - OCR output structure

#### Processing Stages
- **`ingest.py`** - File ingestion
  - Upload handling
  - Submission ID generation
  - Artifact directory creation

- **`ocr.py`** - Text extraction
  - OCR provider protocol
  - Stub OCR implementation
  - Provider factory function

- **`segment.py`** - Text segmentation
  - Contact/essay separation
  - Anchor word detection
  - Layout heuristics

- **`extract.py`** - Field extraction
  - Contact field parsing (regex)
  - Essay metrics computation
  - Optional field handling

- **`validate.py`** - Data validation
  - Required field checking
  - Quality validation
  - Review flag logic

- **`csv_writer.py`** - Data persistence
  - CSV export with frozen headers
  - Dual routing (clean vs review)
  - Statistics tracking

- **`runner.py`** - Pipeline orchestration
  - Stage coordination
  - Artifact generation
  - Processing reports

### Configuration
- **`requirements.txt`** - Python dependencies
- **`.gitignore`** - Git exclusions

## ğŸ“ Directory Structure

```
essayflow/
â”œâ”€â”€ Documentation (you are here)
â”‚   â”œâ”€â”€ INDEX.md              # This file
â”‚   â”œâ”€â”€ QUICKSTART.md         # Quick start
â”‚   â”œâ”€â”€ README.md             # Main docs
â”‚   â”œâ”€â”€ ARCHITECTURE.md       # Technical details
â”‚   â”œâ”€â”€ PROJECT_SUMMARY.md    # Overview
â”‚   â””â”€â”€ TESTING_GUIDE.md      # Testing
â”‚
â”œâ”€â”€ Application
â”‚   â””â”€â”€ app.py                # Streamlit UI
â”‚
â”œâ”€â”€ Pipeline (core logic)
â”‚   â””â”€â”€ pipeline/
â”‚       â”œâ”€â”€ schema.py         # Data models
â”‚       â”œâ”€â”€ ingest.py         # Ingestion
â”‚       â”œâ”€â”€ ocr.py            # OCR
â”‚       â”œâ”€â”€ segment.py        # Segmentation
â”‚       â”œâ”€â”€ extract.py        # Extraction
â”‚       â”œâ”€â”€ validate.py       # Validation
â”‚       â”œâ”€â”€ csv_writer.py     # Export
â”‚       â””â”€â”€ runner.py         # Orchestration
â”‚
â”œâ”€â”€ Configuration
â”‚   â”œâ”€â”€ requirements.txt      # Dependencies
â”‚   â””â”€â”€ .gitignore           # Git rules
â”‚
â””â”€â”€ Runtime (generated)
    â”œâ”€â”€ artifacts/            # Processing artifacts
    â””â”€â”€ outputs/              # CSV exports
```

## ğŸš€ Quick Navigation

### I want to...

**Get started quickly**
â†’ [QUICKSTART.md](QUICKSTART.md)

**Understand the architecture**
â†’ [ARCHITECTURE.md](ARCHITECTURE.md)

**See what was built**
â†’ [PROJECT_SUMMARY.md](PROJECT_SUMMARY.md)

**Learn about testing**
â†’ [TESTING_GUIDE.md](TESTING_GUIDE.md)

**Read full documentation**
â†’ [README.md](README.md)

**Modify the code**
â†’ Start with `pipeline/schema.py` to understand data models
â†’ Then review `pipeline/runner.py` for pipeline flow

**Add a new OCR provider**
â†’ See "Adding New OCR Providers" in [ARCHITECTURE.md](ARCHITECTURE.md)

**Add a new field**
â†’ See "Adding New Fields" in [ARCHITECTURE.md](ARCHITECTURE.md)

**Debug an issue**
â†’ Check artifacts in `artifacts/[submission_id]/`
â†’ Review [TESTING_GUIDE.md](TESTING_GUIDE.md) troubleshooting section

## ğŸ“Š Data Flow Reference

```
Upload â†’ Ingest â†’ OCR â†’ Segment â†’ Extract â†’ Validate â†’ Export
  â†“        â†“       â†“       â†“         â†“         â†“         â†“
Image   Metadata  Text   Blocks   Fields   Record    CSV
```

## ğŸ”‘ Key Concepts

### Submission Record
Complete data for one essay submission including contact info, metrics, and validation status.

### Artifacts
JSON and text files generated at each pipeline stage for debugging and audit trails.

### Validation Flags
Automatic quality checks that route submissions to "clean" or "needs review" CSV files.

### OCR Provider
Abstraction layer for text extraction - currently stub, ready for real OCR integration.

### Review Reason Codes
Semicolon-separated codes indicating why a submission needs manual review:
- `MISSING_NAME`, `MISSING_SCHOOL`, `MISSING_GRADE`
- `EMPTY_ESSAY`, `SHORT_ESSAY`
- `LOW_CONFIDENCE`

## ğŸ¯ Common Tasks

### Run the Application
```bash
streamlit run app.py
```

### Process a Submission
1. Upload image
2. Click "Run Processor"
3. Review results
4. Click "Write to CSV"

### Check Artifacts
```bash
ls artifacts/sub_*/
cat artifacts/sub_*/structured.json
```

### View CSV Output
```bash
cat outputs/submissions_clean.csv
cat outputs/submissions_needs_review.csv
```

### Test Individual Module
```python
from pipeline.segment import split_contact_vs_essay
contact, essay = split_contact_vs_essay(text)
```

## ğŸ”§ Development Workflow

1. **Setup**: Follow [QUICKSTART.md](QUICKSTART.md)
2. **Understand**: Read [ARCHITECTURE.md](ARCHITECTURE.md)
3. **Modify**: Edit pipeline modules
4. **Test**: Use [TESTING_GUIDE.md](TESTING_GUIDE.md)
5. **Run**: `streamlit run app.py`
6. **Debug**: Check artifacts directory

## ğŸ“– Code Reading Order

For new developers, read code in this order:

1. `pipeline/schema.py` - Understand data structures
2. `pipeline/runner.py` - See overall pipeline flow
3. `pipeline/ingest.py` - Simple starting point
4. `pipeline/ocr.py` - OCR abstraction
5. `pipeline/segment.py` - Text processing
6. `pipeline/extract.py` - Field parsing
7. `pipeline/validate.py` - Validation logic
8. `pipeline/csv_writer.py` - Export logic
9. `app.py` - UI integration

## ğŸ› Debugging Checklist

1. Check terminal output for errors
2. Review artifacts in `artifacts/[submission_id]/`
3. Inspect `validation.json` for issues
4. Check CSV files in `outputs/`
5. Verify all pipeline stages completed
6. Review [TESTING_GUIDE.md](TESTING_GUIDE.md) troubleshooting

## ğŸ“ Contributing

When adding features:

1. Update data models in `schema.py` if needed
2. Add processing logic to appropriate module
3. Update validation rules if needed
4. Update CSV headers if adding fields
5. Update UI in `app.py`
6. Document in relevant .md file
7. Add tests (see [TESTING_GUIDE.md](TESTING_GUIDE.md))

## ğŸ“ Learning Resources

### Python Concepts Used
- Pydantic for data validation
- Type hints and protocols
- Pathlib for file operations
- Context managers (with statements)
- List comprehensions

### Streamlit Concepts
- File uploaders
- Session state
- Columns and layout
- Buttons and interactions
- Status indicators

### Design Patterns
- Protocol/Interface pattern (OCR providers)
- Factory pattern (get_ocr_provider)
- Pipeline pattern (staged processing)
- Strategy pattern (validation rules)

## ğŸ“ Support

For issues or questions:

1. Check [QUICKSTART.md](QUICKSTART.md) troubleshooting
2. Review [TESTING_GUIDE.md](TESTING_GUIDE.md) common issues
3. Inspect artifacts for debugging
4. Check documentation for relevant section

## ğŸ—ºï¸ Roadmap

See "Future Enhancements" in [ARCHITECTURE.md](ARCHITECTURE.md):
- Multi-page support
- Batch processing
- ML-based extraction
- Manual review interface
- Analytics dashboard
- API endpoints

---

**Last Updated**: December 2023  
**Version**: 1.0 (Stub OCR Skeleton)  
**Status**: Complete and Ready for OCR Integration


